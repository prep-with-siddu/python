# 7.7 Networking & HTTP ‚Äî Notes

> **Status**: ‚úÖ Completed  
> **Date**: 28 February 2026

---

## 1. `requests` Library Deep Dive

The most popular HTTP library for Python. Synchronous, simple, powerful.

```python
# pip install requests
import requests

# ---- GET ----
response = requests.get("https://api.github.com/users/sidrameshwar")
print(response.status_code)      # 200
print(response.json())           # Parsed JSON ‚Üí dict
print(response.text)             # Raw text
print(response.headers)          # Response headers
print(response.url)              # Final URL (after redirects)
print(response.elapsed)          # Time taken

# ---- POST ----
data = {"name": "New Repo", "private": True}
response = requests.post(
    "https://api.github.com/user/repos",
    json=data,                   # Auto-sets Content-Type: application/json
    headers={"Authorization": "Bearer YOUR_TOKEN"}
)

# ---- PUT / PATCH / DELETE ----
requests.put(url, json=data)
requests.patch(url, json=data)
requests.delete(url)

# ---- Query Parameters ----
response = requests.get(
    "https://api.example.com/users",
    params={"page": 1, "limit": 10, "search": "Sid"}
)
print(response.url)    # https://api.example.com/users?page=1&limit=10&search=Sid

# ---- Form Data ----
response = requests.post(url, data={"username": "sid", "password": "123"})

# ---- File Upload ----
with open("photo.jpg", "rb") as f:
    response = requests.post(url, files={"image": f})
```

### Sessions (Connection Reuse)
```python
import requests

# Session reuses TCP connections ‚Üí MUCH faster for multiple requests!
session = requests.Session()

# Set default headers for all requests
session.headers.update({
    "Authorization": "Bearer YOUR_TOKEN",
    "Content-Type": "application/json",
    "User-Agent": "MyApp/1.0"
})

# All requests now include those headers
users = session.get("https://api.example.com/users")
user = session.get("https://api.example.com/users/1")
orders = session.get("https://api.example.com/orders")

# Session also maintains cookies
session.get("https://site.com/login")    # Gets cookies
session.get("https://site.com/dashboard")  # Sends cookies back
```

### Auth
```python
from requests.auth import HTTPBasicAuth

# Basic Auth
response = requests.get(url, auth=HTTPBasicAuth("user", "pass"))
# Shortcut:
response = requests.get(url, auth=("user", "pass"))

# Bearer Token (most common for APIs)
headers = {"Authorization": "Bearer eyJhbGciOi..."}
response = requests.get(url, headers=headers)

# Custom Auth
from requests.auth import AuthBase

class TokenAuth(AuthBase):
    def __init__(self, token):
        self.token = token
    
    def __call__(self, r):
        r.headers["X-API-Key"] = self.token
        return r

response = requests.get(url, auth=TokenAuth("my-api-key"))
```

### Timeouts
```python
# ALWAYS set timeouts in production! ‚ùó
try:
    response = requests.get(url, timeout=5)          # 5 sec for both
    response = requests.get(url, timeout=(3, 10))    # (connect, read)
except requests.Timeout:
    print("Request timed out!")
except requests.ConnectionError:
    print("Connection failed!")
except requests.HTTPError:
    print("HTTP error!")

# Raise exception for 4xx/5xx responses
response = requests.get(url)
response.raise_for_status()    # Raises HTTPError if status >= 400
```

### Retries with urllib3 Adapter
```python
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Configure retry strategy
retry_strategy = Retry(
    total=3,                    # Max retries
    backoff_factor=1,           # Wait: 1s, 2s, 4s...
    status_forcelist=[429, 500, 502, 503, 504],  # Retry on these
    allowed_methods=["GET", "POST"],
)

adapter = HTTPAdapter(
    max_retries=retry_strategy,
    pool_connections=10,        # Connection pool size
    pool_maxsize=20,            # Max connections
)

session = requests.Session()
session.mount("http://", adapter)
session.mount("https://", adapter)

# Now all requests auto-retry!
response = session.get("https://api.example.com/data")
```

---

## 2. `httpx` ‚Äî Async HTTP Client

Modern replacement for `requests`. Supports both sync AND async.

```python
# pip install httpx
import httpx

# ---- Sync (like requests) ----
response = httpx.get("https://api.github.com")
print(response.status_code)     # 200
print(response.json())

# ---- Async ----
import asyncio

async def fetch_data():
    async with httpx.AsyncClient() as client:
        response = await client.get("https://api.github.com")
        return response.json()

data = asyncio.run(fetch_data())

# ---- Async with session-like client ----
async def fetch_multiple():
    async with httpx.AsyncClient(
        base_url="https://api.example.com",
        headers={"Authorization": "Bearer TOKEN"},
        timeout=10.0,
    ) as client:
        # Parallel requests!
        users, orders = await asyncio.gather(
            client.get("/users"),
            client.get("/orders"),
        )
        return users.json(), orders.json()
```

### httpx Advanced Features
```python
import httpx

# ---- Streaming (large responses) ----
async with httpx.AsyncClient() as client:
    async with client.stream("GET", "https://example.com/big-file") as response:
        async for chunk in response.aiter_bytes():
            process(chunk)

# ---- HTTP/2 Support ----
# pip install httpx[http2]
client = httpx.AsyncClient(http2=True)

# ---- Custom transport (retry, logging) ----
transport = httpx.AsyncHTTPTransport(retries=3)
client = httpx.AsyncClient(transport=transport)
```

### requests vs httpx

| Feature | requests | httpx |
|---------|----------|-------|
| Sync | ‚úÖ | ‚úÖ |
| Async | ‚ùå | ‚úÖ |
| HTTP/2 | ‚ùå | ‚úÖ |
| Streaming | Basic | Full async ‚úÖ |
| API | Standard | requests-compatible |
| Use | Simple scripts | Modern async apps ‚úÖ |

---

## 3. `urllib3` Connection Pooling

Low-level HTTP client (requests uses it internally).

```python
import urllib3

# ---- Basic usage ----
http = urllib3.PoolManager()    # Connection pool
response = http.request("GET", "https://api.example.com")
print(response.status)          # 200
print(response.data.decode())   # Response body

# ---- Connection pooling for one host ----
pool = urllib3.HTTPSConnectionPool(
    "api.example.com",
    maxsize=10,              # 10 connections in pool
    retries=3,
    timeout=urllib3.Timeout(connect=5.0, read=10.0),
)

response = pool.request("GET", "/users")

# ---- Advanced PoolManager ----
http = urllib3.PoolManager(
    num_pools=50,            # Number of host pools
    maxsize=10,              # Connections per host
    retries=urllib3.Retry(total=3, backoff_factor=0.5),
    timeout=urllib3.Timeout(total=30),
)
```

> üí° You rarely use urllib3 directly ‚Äî but understanding it helps debug requests/httpx issues.

---

## 4. Retry Strategies with `tenacity`

The most flexible retry library for Python.

```python
# pip install tenacity
from tenacity import (
    retry,
    stop_after_attempt,
    stop_after_delay,
    wait_exponential,
    wait_fixed,
    retry_if_exception_type,
    before_log,
    after_log,
)
import logging

logger = logging.getLogger(__name__)

# ---- Basic retry ----
@retry(stop=stop_after_attempt(3))
def fetch_data():
    response = requests.get("https://api.example.com/data")
    response.raise_for_status()
    return response.json()

# ---- Exponential backoff ----
@retry(
    stop=stop_after_attempt(5),
    wait=wait_exponential(multiplier=1, min=1, max=60),
    # Waits: 1s, 2s, 4s, 8s, 16s (capped at 60s)
)
def call_api():
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    return response.json()

# ---- Retry only on specific exceptions ----
@retry(
    retry=retry_if_exception_type((requests.Timeout, requests.ConnectionError)),
    stop=stop_after_attempt(3),
    wait=wait_fixed(2),
)
def resilient_call():
    return requests.get(url, timeout=5)

# ---- With logging ----
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    before=before_log(logger, logging.WARNING),
    after=after_log(logger, logging.WARNING),
)
def logged_call():
    return requests.get(url)

# ---- Async retry ----
@retry(stop=stop_after_attempt(3), wait=wait_exponential())
async def async_fetch():
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        response.raise_for_status()
        return response.json()

# ---- Time-based stop ----
@retry(stop=stop_after_delay(30))    # Stop after 30 seconds total
def time_limited_call():
    return requests.get(url)
```

---

## 5. `aiohttp` Client Sessions

For high-performance async HTTP (also a web server framework).

```python
# pip install aiohttp
import aiohttp
import asyncio

# ---- Basic GET ----
async def fetch(url: str) -> dict:
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.json()

# ---- Parallel Requests (FAST!) ----
async def fetch_all():
    urls = [
        "https://api.example.com/users/1",
        "https://api.example.com/users/2",
        "https://api.example.com/users/3",
    ]
    
    async with aiohttp.ClientSession() as session:
        tasks = [session.get(url) for url in urls]
        responses = await asyncio.gather(*tasks)
        
        results = []
        for response in responses:
            results.append(await response.json())
        return results

# ---- Session with defaults ----
async def api_calls():
    headers = {"Authorization": "Bearer TOKEN"}
    timeout = aiohttp.ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(
        base_url="https://api.example.com",
        headers=headers,
        timeout=timeout,
    ) as session:
        # POST
        async with session.post("/users", json={"name": "Sid"}) as resp:
            user = await resp.json()
        
        # GET with params
        async with session.get("/search", params={"q": "python"}) as resp:
            results = await resp.json()

# ---- Streaming Download ----
async def download_file(url: str, path: str):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            with open(path, "wb") as f:
                async for chunk in response.content.iter_chunked(8192):
                    f.write(chunk)
```

---

## 6. Making API Calls from Backend

### Service Layer Pattern
```python
import httpx
from typing import Optional
from pydantic import BaseModel

class PaymentResponse(BaseModel):
    transaction_id: str
    status: str
    amount: float

class PaymentService:
    """External payment API integration."""
    
    def __init__(self, api_key: str, base_url: str):
        self.client = httpx.AsyncClient(
            base_url=base_url,
            headers={"X-API-Key": api_key},
            timeout=30.0,
        )
    
    async def create_payment(self, amount: float, currency: str = "INR") -> PaymentResponse:
        response = await self.client.post(
            "/payments",
            json={"amount": amount, "currency": currency},
        )
        response.raise_for_status()
        return PaymentResponse(**response.json())
    
    async def get_payment(self, txn_id: str) -> Optional[PaymentResponse]:
        response = await self.client.get(f"/payments/{txn_id}")
        if response.status_code == 404:
            return None
        response.raise_for_status()
        return PaymentResponse(**response.json())
    
    async def close(self):
        await self.client.aclose()

# Usage in FastAPI
# payment_service = PaymentService(api_key="...", base_url="...")
# result = await payment_service.create_payment(amount=1500)
```

### Circuit Breaker Pattern
```python
import time
from enum import Enum

class CircuitState(Enum):
    CLOSED = "closed"        # Normal ‚Äî requests go through
    OPEN = "open"            # Broken ‚Äî reject requests immediately
    HALF_OPEN = "half_open"  # Testing ‚Äî allow one request

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.state = CircuitState.CLOSED
        self.last_failure_time = 0
    
    def call(self, func, *args, **kwargs):
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker is OPEN ‚Äî service unavailable")
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e
    
    def _on_success(self):
        self.failures = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        self.failures += 1
        self.last_failure_time = time.time()
        if self.failures >= self.failure_threshold:
            self.state = CircuitState.OPEN

# Usage
breaker = CircuitBreaker(failure_threshold=3, timeout=30)
try:
    result = breaker.call(requests.get, "https://api.example.com/data", timeout=5)
except Exception as e:
    print(f"Service unavailable: {e}")
```

---

## 7. Handling Webhooks

Webhooks = external service POSTs data to YOUR endpoint.

```python
# ---- FastAPI Webhook Handler ----
from fastapi import FastAPI, Request, Header, HTTPException
import hmac
import hashlib

app = FastAPI()

WEBHOOK_SECRET = "your-secret-key"

def verify_signature(payload: bytes, signature: str, secret: str) -> bool:
    """Verify webhook signature (HMAC-SHA256)."""
    expected = hmac.new(
        secret.encode(),
        payload,
        hashlib.sha256
    ).hexdigest()
    return hmac.compare_digest(f"sha256={expected}", signature)

@app.post("/webhooks/github")
async def github_webhook(
    request: Request,
    x_hub_signature_256: str = Header(None),
):
    payload = await request.body()
    
    # 1. Verify signature (SECURITY!)
    if not verify_signature(payload, x_hub_signature_256, WEBHOOK_SECRET):
        raise HTTPException(status_code=401, detail="Invalid signature")
    
    # 2. Parse event
    event = await request.json()
    event_type = request.headers.get("X-GitHub-Event")
    
    # 3. Process based on event type
    if event_type == "push":
        branch = event["ref"].split("/")[-1]
        commits = event["commits"]
        print(f"Push to {branch}: {len(commits)} commits")
    
    elif event_type == "pull_request":
        action = event["action"]
        pr = event["pull_request"]
        print(f"PR {action}: {pr['title']}")
    
    # 4. Return 200 quickly (process async if heavy)
    return {"status": "received"}

# ---- Webhook Best Practices ----
# 1. ALWAYS verify signatures
# 2. Return 200 quickly ‚Äî process async for heavy work
# 3. Make handlers idempotent (same event twice = same result)
# 4. Log webhook payloads for debugging
# 5. Set up retry handling (most services retry on 5xx)
```

### Process Webhooks Async (Background Tasks)
```python
from fastapi import FastAPI, BackgroundTasks
import asyncio

app = FastAPI()

async def process_webhook_heavy(event: dict):
    """Heavy processing done in background."""
    await asyncio.sleep(5)    # Simulate heavy work
    print(f"Processed event: {event['id']}")

@app.post("/webhooks/payment")
async def payment_webhook(
    request: Request,
    background_tasks: BackgroundTasks,
):
    event = await request.json()
    
    # Quick response ‚Äî process later
    background_tasks.add_task(process_webhook_heavy, event)
    
    return {"status": "accepted"}    # Return 200 immediately
```

---

## 8. Rate Limiting Outbound Requests

Don't hit external APIs too fast!

```python
import asyncio
import time
from collections import deque

# ---- Simple Rate Limiter ----
class RateLimiter:
    def __init__(self, max_calls: int, period: float):
        """Allow max_calls in period seconds."""
        self.max_calls = max_calls
        self.period = period
        self.calls: deque = deque()
    
    async def acquire(self):
        now = time.monotonic()
        
        # Remove old calls outside the window
        while self.calls and self.calls[0] < now - self.period:
            self.calls.popleft()
        
        if len(self.calls) >= self.max_calls:
            # Wait until oldest call expires
            sleep_time = self.calls[0] + self.period - now
            await asyncio.sleep(sleep_time)
        
        self.calls.append(time.monotonic())

# Usage
rate_limiter = RateLimiter(max_calls=10, period=1.0)    # 10 req/sec

async def fetch_with_limit(url: str):
    await rate_limiter.acquire()
    async with httpx.AsyncClient() as client:
        return await client.get(url)

# ---- Token Bucket (more flexible) ----
class TokenBucket:
    def __init__(self, rate: float, capacity: int):
        self.rate = rate              # Tokens per second
        self.capacity = capacity      # Max tokens
        self.tokens = capacity
        self.last_refill = time.monotonic()
    
    async def acquire(self, tokens: int = 1):
        while True:
            self._refill()
            if self.tokens >= tokens:
                self.tokens -= tokens
                return
            # Wait for enough tokens
            wait_time = (tokens - self.tokens) / self.rate
            await asyncio.sleep(wait_time)
    
    def _refill(self):
        now = time.monotonic()
        elapsed = now - self.last_refill
        self.tokens = min(self.capacity, self.tokens + elapsed * self.rate)
        self.last_refill = now

# 100 requests/minute
bucket = TokenBucket(rate=100/60, capacity=100)

# ---- Using aiohttp with semaphore (simplest approach) ----
async def fetch_many(urls: list[str], max_concurrent: int = 5):
    """Limit concurrent requests."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def fetch_one(url):
        async with semaphore:
            async with httpx.AsyncClient() as client:
                return await client.get(url)
    
    return await asyncio.gather(*[fetch_one(url) for url in urls])
```

### Handling 429 (Too Many Requests)
```python
import httpx
from tenacity import retry, wait_exponential, retry_if_result

def is_rate_limited(response):
    return response.status_code == 429

@retry(
    retry=retry_if_result(is_rate_limited),
    wait=wait_exponential(multiplier=1, min=1, max=60),
)
async def fetch_with_429_handling(client: httpx.AsyncClient, url: str):
    response = await client.get(url)
    
    if response.status_code == 429:
        # Check Retry-After header
        retry_after = int(response.headers.get("Retry-After", 5))
        await asyncio.sleep(retry_after)
    
    return response
```

---

## üîó Quick Reference

```python
# ---- requests (sync) ----
import requests
r = requests.get(url, params={}, headers={}, timeout=10)
r = requests.post(url, json={}, data={}, files={})
r.json()  |  r.text  |  r.status_code  |  r.raise_for_status()

# ---- httpx (sync + async) ----
import httpx
async with httpx.AsyncClient(base_url=url) as client:
    r = await client.get("/path")

# ---- aiohttp (async) ----
import aiohttp
async with aiohttp.ClientSession() as session:
    async with session.get(url) as r:
        data = await r.json()

# ---- tenacity (retry) ----
from tenacity import retry, stop_after_attempt, wait_exponential
@retry(stop=stop_after_attempt(3), wait=wait_exponential())
def call(): ...

# ---- Rate limiting ----
semaphore = asyncio.Semaphore(10)  # Max 10 concurrent
async with semaphore:
    await client.get(url)
```

---

> üìù **Phase 7 Complete!** Next: Phase 8 ‚Äî Git, Testing & Code Quality
